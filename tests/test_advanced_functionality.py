#!/usr/bin/env python3
"""
Comprehensive Test Suite for Remaining pyHMSSQL Functionality

This test suite covers functionality that hasn't been thoroughly tested yet:
- Advanced query optimization scenarios
- Complex transaction management
- Error recovery mechanisms
- Memory management and cleanup
- Advanced indexing strategies
- Query plan caching and invalidation
- Statistics collection and usage
"""

import pytest
import tempfile
import shutil
import time
import gc
import psutil
import os
import sys
import threading
from unittest.mock import Mock, patch
from concurrent.futures import ThreadPoolExecutor

# Add server directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../server"))

from catalog_manager import CatalogManager
from ddl_processor.index_manager import IndexManager
from execution_engine import ExecutionEngine
from planner import Planner
from parser import SQLGlotParser


class TestAdvancedQueryOptimization:
    """Test advanced query optimization scenarios"""

    @pytest.fixture(autouse=True)
    def setup(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix="pyhmssql_advanced_")
        self.catalog_manager = CatalogManager(data_dir=self.test_dir)
        self.index_manager = IndexManager(self.catalog_manager)
        self.planner = Planner(self.catalog_manager, self.index_manager)
        self.parser = SQLGlotParser()
        self.execution_engine = ExecutionEngine(
            self.catalog_manager, self.index_manager, self.planner
        )

        # Create test database
        self.execution_engine.execute(
            {"type": "CREATE_DATABASE", "database": "test_advanced"}
        )
        self.execution_engine.execute(
            {"type": "USE_DATABASE", "database": "test_advanced"}
        )

        yield

        # Cleanup
        try:
            if hasattr(self, "test_dir") and os.path.exists(self.test_dir):
                shutil.rmtree(self.test_dir)
        except:
            pass

    def test_query_plan_caching(self):
        """Test query plan caching and reuse"""

        # Create table for testing
        create_sql = """
        CREATE TABLE cache_test (
            id INTEGER PRIMARY KEY,
            name VARCHAR(100),
            category VARCHAR(50),
            value INTEGER
        )
        """

        parsed = self.parser.parse_sql(create_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        # Insert test data
        for i in range(20):
            insert_sql = (
                f"INSERT INTO cache_test VALUES ({i}, 'Name{i}', 'Cat{i%5}', {i*10})"
            )
            parsed = self.parser.parse_sql(insert_sql)
            plan = self.planner.plan_query(parsed)
            result = self.execution_engine.execute(plan)
            assert result["status"] == "success"

        # Test that similar queries can reuse plans
        similar_queries = [
            "SELECT * FROM cache_test WHERE category = 'Cat1'",
            "SELECT * FROM cache_test WHERE category = 'Cat2'",
            "SELECT * FROM cache_test WHERE category = 'Cat3'",
        ]

        plan_times = []
        for query in similar_queries:
            start_time = time.time()
            parsed = self.parser.parse_sql(query)
            plan = self.planner.plan_query(parsed)
            plan_time = time.time() - start_time
            plan_times.append(plan_time)

            result = self.execution_engine.execute(plan)
            assert result["status"] == "success"

        # Later queries should be faster due to plan caching/reuse
        # (This is more of a performance characteristic test)
        assert all(t < 1.0 for t in plan_times)  # All should be reasonable

    def test_complex_join_optimization(self):
        """Test optimization of complex multi-table joins"""

        # Create multiple tables for complex joins
        tables = {
            "customers": """
                CREATE TABLE customers (
                    id INTEGER PRIMARY KEY,
                    name VARCHAR(100),
                    region VARCHAR(50)
                )
            """,
            "orders": """
                CREATE TABLE orders (
                    id INTEGER PRIMARY KEY,
                    customer_id INTEGER,
                    order_date DATE,
                    status VARCHAR(20)
                )
            """,
            "order_items": """
                CREATE TABLE order_items (
                    id INTEGER PRIMARY KEY,
                    order_id INTEGER,
                    product_id INTEGER,
                    quantity INTEGER,
                    price DECIMAL(10,2)
                )
            """,
            "products": """
                CREATE TABLE products (
                    id INTEGER PRIMARY KEY,
                    name VARCHAR(100),
                    category VARCHAR(50),
                    price DECIMAL(10,2)
                )
            """,
        }

        # Create all tables
        for table_name, sql in tables.items():
            parsed = self.parser.parse_sql(sql)
            plan = self.planner.plan_query(parsed)
            result = self.execution_engine.execute(plan)
            assert result["status"] == "success"

        # Insert sample data
        sample_data = {
            "customers": [
                "(1, 'Customer A', 'North')",
                "(2, 'Customer B', 'South')",
                "(3, 'Customer C', 'East')",
            ],
            "orders": [
                "(1, 1, '2023-01-01', 'completed')",
                "(2, 2, '2023-01-02', 'pending')",
                "(3, 1, '2023-01-03', 'completed')",
            ],
            "order_items": [
                "(1, 1, 1, 2, 10.00)",
                "(2, 1, 2, 1, 20.00)",
                "(3, 2, 1, 3, 10.00)",
                "(4, 3, 3, 1, 50.00)",
            ],
            "products": [
                "(1, 'Product A', 'Electronics', 10.00)",
                "(2, 'Product B', 'Electronics', 20.00)",
                "(3, 'Product C', 'Books', 50.00)",
            ],
        }

        for table_name, values in sample_data.items():
            for value in values:
                insert_sql = f"INSERT INTO {table_name} VALUES {value}"
                parsed = self.parser.parse_sql(insert_sql)
                plan = self.planner.plan_query(parsed)
                result = self.execution_engine.execute(plan)
                assert result["status"] == "success"

        # Create indexes for join optimization
        indexes = [
            "CREATE INDEX idx_customer_id ON orders(customer_id)",
            "CREATE INDEX idx_order_id ON order_items(order_id)",
            "CREATE INDEX idx_product_id ON order_items(product_id)",
        ]

        for index_sql in indexes:
            parsed = self.parser.parse_sql(index_sql)
            plan = self.planner.plan_query(parsed)
            result = self.execution_engine.execute(plan)
            assert result["status"] == "success"

        # Complex multi-table join query
        complex_join_sql = """
        SELECT 
            c.name as customer_name,
            c.region,
            o.order_date,
            p.name as product_name,
            p.category,
            oi.quantity,
            oi.price,
            (oi.quantity * oi.price) as total_amount
        FROM customers c
        JOIN orders o ON c.id = o.customer_id
        JOIN order_items oi ON o.id = oi.order_id
        JOIN products p ON oi.product_id = p.id
        WHERE o.status = 'completed'
        AND p.category = 'Electronics'
        ORDER BY c.name, o.order_date
        """

        start_time = time.time()
        parsed = self.parser.parse_sql(complex_join_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        execution_time = time.time() - start_time

        assert result["status"] == "success"
        assert len(result["rows"]) >= 1  # Should have some results
        assert execution_time < 2.0  # Should be reasonably fast

    def test_subquery_optimization(self):
        """Test optimization of complex subqueries"""

        # Create table for subquery testing
        create_sql = """
        CREATE TABLE sales (
            id INTEGER PRIMARY KEY,
            salesperson VARCHAR(100),
            region VARCHAR(50),
            amount DECIMAL(10,2),
            sale_date DATE
        )
        """

        parsed = self.parser.parse_sql(create_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        # Insert test data
        sales_data = [
            (1, "Alice", "North", 1000.00, "2023-01-01"),
            (2, "Bob", "South", 1500.00, "2023-01-02"),
            (3, "Alice", "North", 2000.00, "2023-01-03"),
            (4, "Charlie", "East", 1200.00, "2023-01-04"),
            (5, "Bob", "South", 1800.00, "2023-01-05"),
        ]

        for sale in sales_data:
            insert_sql = f"INSERT INTO sales VALUES {sale}"
            parsed = self.parser.parse_sql(insert_sql)
            plan = self.planner.plan_query(parsed)
            result = self.execution_engine.execute(plan)
            assert result["status"] == "success"

        # Complex subquery with EXISTS
        subquery_sql = """
        SELECT salesperson, region, SUM(amount) as total_sales
        FROM sales s1
        WHERE EXISTS (
            SELECT 1 FROM sales s2 
            WHERE s2.salesperson = s1.salesperson 
            AND s2.amount > 1500
        )
        GROUP BY salesperson, region
        HAVING SUM(amount) > 2000
        """

        try:
            parsed = self.parser.parse_sql(subquery_sql)
            plan = self.planner.plan_query(parsed)
            result = self.execution_engine.execute(plan)

            # Subqueries might not be fully implemented
            if result["status"] == "success":
                assert len(result["rows"]) >= 0
            else:
                # Should handle gracefully if not implemented
                assert (
                    "not implemented" in result.get("error", "").lower()
                    or "not supported" in result.get("error", "").lower()
                )
        except Exception:
            # Complex subqueries might not be fully supported
            pass

    def test_index_selection_optimization(self):
        """Test optimal index selection for queries"""

        # Create table with multiple potential indexes
        create_sql = """
        CREATE TABLE index_test (
            id INTEGER PRIMARY KEY,
            first_name VARCHAR(50),
            last_name VARCHAR(50),
            email VARCHAR(100),
            age INTEGER,
            salary DECIMAL(10,2),
            department VARCHAR(50),
            hire_date DATE
        )
        """

        parsed = self.parser.parse_sql(create_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        # Insert test data
        for i in range(50):
            insert_sql = f"""
            INSERT INTO index_test VALUES (
                {i}, 
                'First{i}', 
                'Last{i}', 
                'user{i}@example.com',
                {25 + i % 40},
                {50000 + i * 1000},
                'Dept{i % 5}',
                '2020-01-01'
            )
            """
            parsed = self.parser.parse_sql(insert_sql)
            plan = self.planner.plan_query(parsed)
            result = self.execution_engine.execute(plan)
            assert result["status"] == "success"

        # Create multiple indexes
        indexes = [
            "CREATE INDEX idx_first_name ON index_test(first_name)",
            "CREATE INDEX idx_last_name ON index_test(last_name)",
            "CREATE INDEX idx_email ON index_test(email)",
            "CREATE INDEX idx_age ON index_test(age)",
            "CREATE INDEX idx_department ON index_test(department)",
            "CREATE INDEX idx_salary ON index_test(salary)",
        ]

        for index_sql in indexes:
            parsed = self.parser.parse_sql(index_sql)
            plan = self.planner.plan_query(parsed)
            result = self.execution_engine.execute(plan)
            assert result["status"] == "success"

        # Test queries that should select different optimal indexes
        test_queries = [
            "SELECT * FROM index_test WHERE first_name = 'First10'",  # Should use idx_first_name
            "SELECT * FROM index_test WHERE age BETWEEN 30 AND 40",  # Should use idx_age
            "SELECT * FROM index_test WHERE department = 'Dept1'",  # Should use idx_department
            "SELECT * FROM index_test WHERE salary > 60000",  # Should use idx_salary
        ]

        for query in test_queries:
            start_time = time.time()
            parsed = self.parser.parse_sql(query)
            plan = self.planner.plan_query(parsed)
            result = self.execution_engine.execute(plan)
            query_time = time.time() - start_time

            assert result["status"] == "success"
            assert query_time < 1.0  # Should be fast with proper index selection


class TestAdvancedTransactionManagement:
    """Test advanced transaction management scenarios"""

    @pytest.fixture(autouse=True)
    def setup(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix="pyhmssql_transaction_")
        self.catalog_manager = CatalogManager(data_dir=self.test_dir)
        self.index_manager = IndexManager(self.catalog_manager)
        self.planner = Planner(self.catalog_manager, self.index_manager)
        self.parser = SQLGlotParser()
        self.execution_engine = ExecutionEngine(
            self.catalog_manager, self.index_manager, self.planner
        )

        # Create test database
        self.execution_engine.execute(
            {"type": "CREATE_DATABASE", "database": "test_transaction"}
        )
        self.execution_engine.execute(
            {"type": "USE_DATABASE", "database": "test_transaction"}
        )

        yield

        # Cleanup
        try:
            if hasattr(self, "test_dir") and os.path.exists(self.test_dir):
                shutil.rmtree(self.test_dir)
        except:
            pass

    def test_nested_transaction_simulation(self):
        """Test nested transaction behavior (savepoints simulation)"""

        # Create table for transaction testing
        create_sql = """
        CREATE TABLE account (
            id INTEGER PRIMARY KEY,
            name VARCHAR(100),
            balance DECIMAL(10,2)
        )
        """

        parsed = self.parser.parse_sql(create_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        # Insert initial data
        insert_sql = """
        INSERT INTO account (id, name, balance) VALUES 
            (1, 'Account A', 1000.00),
            (2, 'Account B', 500.00)
        """

        parsed = self.parser.parse_sql(insert_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        # Simulate nested transaction with manual savepoint tracking
        transaction_states = []

        # Begin outer transaction
        begin_sql = "BEGIN TRANSACTION"
        parsed = self.parser.parse_sql(begin_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)

        # Save state (simulate savepoint)
        select_sql = "SELECT id, balance FROM account ORDER BY id"
        parsed = self.parser.parse_sql(select_sql)
        plan = self.planner.plan_query(parsed)
        initial_state = self.execution_engine.execute(plan)
        transaction_states.append(initial_state["rows"])

        # Perform some operations
        update1_sql = "UPDATE account SET balance = balance - 100 WHERE id = 1"
        update2_sql = "UPDATE account SET balance = balance + 100 WHERE id = 2"

        for sql in [update1_sql, update2_sql]:
            parsed = self.parser.parse_sql(sql)
            plan = self.planner.plan_query(parsed)
            result = self.execution_engine.execute(plan)
            assert result["status"] == "success"

        # Check intermediate state
        parsed = self.parser.parse_sql(select_sql)
        plan = self.planner.plan_query(parsed)
        intermediate_state = self.execution_engine.execute(plan)

        assert intermediate_state["status"] == "success"
        # Account A should have 900, Account B should have 600

        # Commit transaction
        commit_sql = "COMMIT"
        parsed = self.parser.parse_sql(commit_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)

        # Verify final state
        parsed = self.parser.parse_sql(select_sql)
        plan = self.planner.plan_query(parsed)
        final_state = self.execution_engine.execute(plan)

        assert final_state["status"] == "success"
        assert len(final_state["rows"]) == 2

    def test_deadlock_detection_simulation(self):
        """Test deadlock detection and resolution"""

        # Create table for deadlock testing
        create_sql = """
        CREATE TABLE resource (
            id INTEGER PRIMARY KEY,
            name VARCHAR(100),
            locked_by VARCHAR(100)
        )
        """

        parsed = self.parser.parse_sql(create_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        # Insert test resources
        insert_sql = """
        INSERT INTO resource (id, name, locked_by) VALUES
            (1, 'Resource A', NULL),
            (2, 'Resource B', NULL)
        """

        parsed = self.parser.parse_sql(insert_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        # Simulate concurrent transactions that could deadlock
        def transaction_1():
            try:
                # Lock Resource A
                update1 = "UPDATE resource SET locked_by = 'Transaction1' WHERE id = 1"
                parsed = self.parser.parse_sql(update1)
                plan = self.planner.plan_query(parsed)
                result = self.execution_engine.execute(plan)

                time.sleep(0.1)  # Small delay to simulate work

                # Try to lock Resource B
                update2 = "UPDATE resource SET locked_by = 'Transaction1' WHERE id = 2"
                parsed = self.parser.parse_sql(update2)
                plan = self.planner.plan_query(parsed)
                result = self.execution_engine.execute(plan)

                return result["status"] == "success"
            except Exception:
                return False

        def transaction_2():
            try:
                # Lock Resource B
                update1 = "UPDATE resource SET locked_by = 'Transaction2' WHERE id = 2"
                parsed = self.parser.parse_sql(update1)
                plan = self.planner.plan_query(parsed)
                result = self.execution_engine.execute(plan)

                time.sleep(0.1)  # Small delay to simulate work

                # Try to lock Resource A
                update2 = "UPDATE resource SET locked_by = 'Transaction2' WHERE id = 1"
                parsed = self.parser.parse_sql(update2)
                plan = self.planner.plan_query(parsed)
                result = self.execution_engine.execute(plan)

                return result["status"] == "success"
            except Exception:
                return False

        # Run concurrent transactions
        with ThreadPoolExecutor(max_workers=2) as executor:
            future1 = executor.submit(transaction_1)
            future2 = executor.submit(transaction_2)

            result1 = future1.result()
            result2 = future2.result()

        # At least one transaction should complete successfully
        # (Proper deadlock detection would allow this)
        assert result1 or result2

    def test_long_running_transaction_behavior(self):
        """Test behavior of long-running transactions"""

        # Create table for long transaction testing
        create_sql = """
        CREATE TABLE long_test (
            id INTEGER PRIMARY KEY,
            data VARCHAR(1000),
            timestamp DATETIME
        )
        """

        parsed = self.parser.parse_sql(create_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        # Begin long transaction
        begin_sql = "BEGIN TRANSACTION"
        parsed = self.parser.parse_sql(begin_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)

        # Perform many operations within the transaction
        num_operations = 50  # Reduced for CI environment
        successful_operations = 0

        for i in range(num_operations):
            insert_sql = f"INSERT INTO long_test VALUES ({i}, 'Data{i}', '2023-01-01')"
            try:
                parsed = self.parser.parse_sql(insert_sql)
                plan = self.planner.plan_query(parsed)
                result = self.execution_engine.execute(plan)
                if result["status"] == "success":
                    successful_operations += 1
            except Exception:
                # Some operations might fail due to resource constraints
                pass

        # Commit the transaction
        commit_sql = "COMMIT"
        parsed = self.parser.parse_sql(commit_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)

        # Verify that most operations succeeded
        select_sql = "SELECT COUNT(*) FROM long_test"
        parsed = self.parser.parse_sql(select_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)

        assert result["status"] == "success"
        committed_count = result["rows"][0][0]

        # Should have committed most of the operations
        assert committed_count >= num_operations * 0.8


class TestMemoryAndResourceManagement:
    """Test memory usage and resource management"""

    @pytest.fixture(autouse=True)
    def setup(self):
        """Set up test environment"""
        self.test_dir = tempfile.mkdtemp(prefix="pyhmssql_memory_")
        self.catalog_manager = CatalogManager(data_dir=self.test_dir)
        self.index_manager = IndexManager(self.catalog_manager)
        self.planner = Planner(self.catalog_manager, self.index_manager)
        self.parser = SQLGlotParser()
        self.execution_engine = ExecutionEngine(
            self.catalog_manager, self.index_manager, self.planner
        )

        # Create test database
        self.execution_engine.execute(
            {"type": "CREATE_DATABASE", "database": "test_memory"}
        )
        self.execution_engine.execute(
            {"type": "USE_DATABASE", "database": "test_memory"}
        )

        yield

        # Cleanup
        try:
            if hasattr(self, "test_dir") and os.path.exists(self.test_dir):
                shutil.rmtree(self.test_dir)
        except:
            pass

    def test_memory_usage_under_load(self):
        """Test memory usage under heavy load"""

        # Get initial memory usage
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # Create table for memory testing
        create_sql = """
        CREATE TABLE memory_test (
            id INTEGER PRIMARY KEY,
            data VARCHAR(1000)
        )
        """

        parsed = self.parser.parse_sql(create_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        # Insert many records
        num_records = 500  # Reduced for CI environment
        batch_size = 50

        for batch_start in range(0, num_records, batch_size):
            batch_values = []
            for i in range(batch_start, min(batch_start + batch_size, num_records)):
                data_content = "x" * 100  # 100 character string
                batch_values.append(f"({i}, '{data_content}')")

            if batch_values:
                insert_sql = f"INSERT INTO memory_test VALUES {', '.join(batch_values)}"
                try:
                    parsed = self.parser.parse_sql(insert_sql)
                    plan = self.planner.plan_query(parsed)
                    result = self.execution_engine.execute(plan)
                    assert result["status"] == "success"
                except Exception:
                    # Some batches might fail due to memory constraints
                    pass

        # Check memory usage after operations
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB

        # Force garbage collection
        gc.collect()
        time.sleep(0.1)

        # Check memory after cleanup
        final_memory = process.memory_info().rss / 1024 / 1024  # MB

        # Memory should not grow excessively
        memory_growth = peak_memory - initial_memory
        assert memory_growth < 500  # Should not use more than 500MB additional

        # Some memory should be reclaimed after GC
        memory_reclaimed = peak_memory - final_memory
        assert memory_reclaimed >= 0  # Should not increase

    def test_resource_cleanup_after_errors(self):
        """Test that resources are properly cleaned up after errors"""

        # Create table for error testing
        create_sql = """
        CREATE TABLE error_test (
            id INTEGER PRIMARY KEY,
            value INTEGER NOT NULL
        )
        """

        parsed = self.parser.parse_sql(create_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        # Get initial resource state
        initial_open_files = len(psutil.Process().open_files())

        # Cause various types of errors
        error_scenarios = [
            "INSERT INTO error_test VALUES (1, NULL)",  # NULL constraint violation
            "INSERT INTO error_test VALUES (1, 100)",  # Duplicate primary key (after first)
            "SELECT * FROM nonexistent_table",  # Table doesn't exist
            "INSERT INTO error_test VALUES ('invalid', 100)",  # Type mismatch
        ]

        # First insert a valid record
        valid_insert = "INSERT INTO error_test VALUES (1, 100)"
        parsed = self.parser.parse_sql(valid_insert)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        error_count = 0
        for error_sql in error_scenarios:
            try:
                parsed = self.parser.parse_sql(error_sql)
                plan = self.planner.plan_query(parsed)
                result = self.execution_engine.execute(plan)

                if result["status"] == "error":
                    error_count += 1
            except Exception:
                error_count += 1

        # Most scenarios should produce errors
        assert error_count >= len(error_scenarios) * 0.7

        # Check that resources weren't leaked
        final_open_files = len(psutil.Process().open_files())
        file_leak = final_open_files - initial_open_files
        assert file_leak <= 5  # Allow small number of additional files

        # Verify system is still functional after errors
        test_sql = "SELECT COUNT(*) FROM error_test"
        parsed = self.parser.parse_sql(test_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

    def test_concurrent_resource_management(self):
        """Test resource management under concurrent access"""

        # Create table for concurrent testing
        create_sql = """
        CREATE TABLE concurrent_resource_test (
            id INTEGER PRIMARY KEY,
            thread_id INTEGER,
            data VARCHAR(100)
        )
        """

        parsed = self.parser.parse_sql(create_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)
        assert result["status"] == "success"

        def worker_thread(thread_id, operations_count):
            """Worker function for concurrent operations"""
            successful_ops = 0

            for i in range(operations_count):
                try:
                    # Mix of different operations
                    if i % 3 == 0:
                        # Insert operation
                        sql = f"INSERT INTO concurrent_resource_test VALUES ({thread_id * 1000 + i}, {thread_id}, 'Data{i}')"
                    elif i % 3 == 1:
                        # Select operation
                        sql = f"SELECT COUNT(*) FROM concurrent_resource_test WHERE thread_id = {thread_id}"
                    else:
                        # Update operation
                        sql = f"UPDATE concurrent_resource_test SET data = 'Updated{i}' WHERE id = {thread_id * 1000 + i - 1}"

                    parsed = self.parser.parse_sql(sql)
                    plan = self.planner.plan_query(parsed)
                    result = self.execution_engine.execute(plan)

                    if result["status"] == "success":
                        successful_ops += 1

                except Exception:
                    # Some operations might fail due to concurrency
                    pass

            return successful_ops

        # Run concurrent workers
        num_threads = 3
        operations_per_thread = 20

        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [
                executor.submit(worker_thread, thread_id, operations_per_thread)
                for thread_id in range(num_threads)
            ]

            results = [future.result() for future in futures]

        # Most operations should succeed
        total_successful = sum(results)
        total_attempted = num_threads * operations_per_thread
        success_rate = total_successful / total_attempted

        assert success_rate >= 0.6  # At least 60% success rate

        # Verify system integrity after concurrent operations
        verify_sql = "SELECT COUNT(*) FROM concurrent_resource_test"
        parsed = self.parser.parse_sql(verify_sql)
        plan = self.planner.plan_query(parsed)
        result = self.execution_engine.execute(plan)

        assert result["status"] == "success"
        final_count = result["rows"][0][0]
        assert final_count >= 0  # Should have some records


if __name__ == "__main__":
    # Can be run standalone for debugging
    print("Running advanced functionality tests...")

    # Test query optimization
    print("Testing query optimization...")
    opt_test = TestAdvancedQueryOptimization()
    opt_test.setup()
    try:
        opt_test.test_query_plan_caching()
        print("‚úÖ Query plan caching test passed")

        opt_test.test_complex_join_optimization()
        print("‚úÖ Complex join optimization test passed")

        opt_test.test_index_selection_optimization()
        print("‚úÖ Index selection optimization test passed")
    except Exception as e:
        print(f"‚ùå Query optimization test failed: {e}")

    # Test transaction management
    print("Testing transaction management...")
    trans_test = TestAdvancedTransactionManagement()
    trans_test.setup()
    try:
        trans_test.test_nested_transaction_simulation()
        print("‚úÖ Nested transaction test passed")

        trans_test.test_long_running_transaction_behavior()
        print("‚úÖ Long running transaction test passed")
    except Exception as e:
        print(f"‚ùå Transaction management test failed: {e}")

    # Test memory management
    print("Testing memory management...")
    mem_test = TestMemoryAndResourceManagement()
    mem_test.setup()
    try:
        mem_test.test_memory_usage_under_load()
        print("‚úÖ Memory usage test passed")

        mem_test.test_resource_cleanup_after_errors()
        print("‚úÖ Resource cleanup test passed")
    except Exception as e:
        print(f"‚ùå Memory management test failed: {e}")

    print("üéâ Advanced functionality tests completed!")
