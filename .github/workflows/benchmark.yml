name: Performance Benchmarks

on:
  push:
    branches: [main]
    paths:
      - "server/**/*.py"
      - "server/**/*.pyx"
  pull_request:
    branches: [main]
    paths:
      - "server/**/*.py"
      - "server/**/*.pyx"
  schedule:
    # Run benchmarks weekly on Sundays at 2 AM UTC
    - cron: "0 2 * * 0"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
          cache: "pip"

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y graphviz graphviz-dev gcc

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install pytest-benchmark memory-profiler psutil

      - name: Build Cython extensions
        run: |
          cd server
          if [ -f "build_bptree.sh" ]; then
            chmod +x build_bptree.sh
            ./build_bptree.sh || echo "Cython build failed, using Python fallback"
          fi

      - name: Run performance benchmarks
        run: |
          pytest tests/ -k "performance or benchmark" \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-sort=mean \
            --benchmark-min-rounds=5

      - name: Generate performance report
        run: |
          python server/benchmark.py --output-format json > detailed-benchmark.json
        continue-on-error: true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            benchmark-results.json
            detailed-benchmark.json

      - name: Compare with baseline (PR only)
        if: github.event_name == 'pull_request'
        run: |
          echo "Performance comparison would go here"
          echo "This could compare current results with main branch baseline"
        continue-on-error: true

      - name: Comment benchmark results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('benchmark-results.json')) {
              const results = JSON.parse(fs.readFileSync('benchmark-results.json', 'utf8'));
              
              let comment = '## ðŸ“Š Performance Benchmark Results\n\n';
              comment += '| Test | Mean Time | Std Dev | Min Time | Max Time |\n';
              comment += '|------|-----------|---------|----------|----------|\n';
              
              if (results.benchmarks) {
                results.benchmarks.forEach(benchmark => {
                  const stats = benchmark.stats;
                  comment += `| ${benchmark.name} | ${stats.mean.toFixed(4)}s | ${stats.stddev.toFixed(4)}s | ${stats.min.toFixed(4)}s | ${stats.max.toFixed(4)}s |\n`;
                });
              }
              
              comment += '\n*Benchmarks run on Ubuntu latest with Python 3.13*';
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }
        continue-on-error: true
